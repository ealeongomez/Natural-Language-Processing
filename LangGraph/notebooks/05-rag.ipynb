{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aac6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG Simple - Processing the Paper \"Attention Is All You Need\"\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042ae937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The Transformer architecture is a type of neural network architecture that was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. It has become a foundational model for various natural language processing (NLP) tasks and has also been adapted for other domains such as computer vision and audio processing. The key features of the Transformer architecture include:\n",
      "\n",
      "### 1. **Self-Attention Mechanism:**\n",
      "   - The core innovation of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence relative to each other. This enables the model to capture contextual relationships without relying on sequential processing, as seen in recurrent neural networks (RNNs).\n",
      "\n",
      "### 2. **Positional Encoding:**\n",
      "   - Since Transformers do not have a built-in notion of sequence order (unlike RNNs), they use positional encodings to inject information about the position of each token in the input sequence. This allows the model to understand the order of words.\n",
      "\n",
      "### 3. **Multi-Head Attention:**\n",
      "   - The self-attention mechanism is implemented in multiple \"heads,\" allowing the model to focus on different parts of the input simultaneously. Each head learns different representations, which are then concatenated and linearly transformed.\n",
      "\n",
      "### 4. **Feed-Forward Neural Networks:**\n",
      "   - After the attention layers, the output is passed through feed-forward neural networks (FFNNs) that are applied independently to each position. These networks typically consist of two linear transformations with a non-linear activation function in between.\n",
      "\n",
      "### 5. **Layer Normalization and Residual Connections:**\n",
      "   - Each sub-layer (attention and feed-forward) is followed by layer normalization and a residual connection, which helps in stabilizing the training and allows for better gradient flow.\n",
      "\n",
      "### 6. **Encoder-Decoder Structure:**\n",
      "   - The original Transformer architecture consists of an encoder and a decoder:\n",
      "     - **Encoder:** Composed of multiple identical layers, each containing a multi-head self-attention mechanism and a feed-forward network. The encoder processes the input sequence and generates a set of continuous representations.\n",
      "     - **Decoder:** Also composed of multiple identical layers, but it includes an additional multi-head attention mechanism that attends to the encoder's output. The decoder generates the output sequence one token at a time.\n",
      "\n",
      "### 7. **Scalability and Parallelization:**\n",
      "   - Unlike RNNs, which process sequences sequentially, Transformers can process entire sequences in parallel, making them more efficient for training on large datasets.\n",
      "\n",
      "### Applications:\n",
      "Transformers have been widely adopted in various NLP tasks, including machine translation, text summarization, sentiment analysis, and more. They are also the backbone of many state-of-the-art models, such as BERT, GPT, and T5.\n",
      "\n",
      "Overall, the Transformer architecture has revolutionized the field of deep learning, particularly in NLP, due to its effectiveness and efficiency in handling sequential data.\n"
     ]
    }
   ],
   "source": [
    "llm = init_chat_model(model=\"gpt-4o-mini\", temperature=0)\n",
    "response = llm.invoke(\"What is the Transformer architecture?\")\n",
    "\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b73761",
   "metadata": {},
   "source": [
    "# **Build RAG** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1c630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup RAG\n",
    "cache = \"../faiss_cache/transformer_paper\"\n",
    "\n",
    "# Cargar y procesar PDF\n",
    "docs = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(\n",
    "    PyPDFLoader(\"../pdfs/Paper.pdf\").load()\n",
    ")\n",
    "\n",
    "# Vector store con cache\n",
    "if os.path.exists(cache + \".faiss\"):\n",
    "    vectorstore = FAISS.load_local(cache, OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "    vectorstore.save_local(cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d03f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG listo\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Bas√°ndote en el paper 'Attention Is All You Need':\\n\\nContexto: {context}\\n\\nPregunta: {question}\\n\\nRespuesta:\"\n",
    ")\n",
    "\n",
    "rag = (\n",
    "    {\"context\": retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)), \"question\": RunnablePassthrough()}\n",
    "    | prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG listo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0a5d9",
   "metadata": {},
   "source": [
    "## üîç **Make questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07fdea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Transformer architecture is a novel model design introduced in the paper \"Attention Is All You Need,\" which relies entirely on attention mechanisms rather than recurrence or convolutions. This architecture is particularly effective for tasks such as machine translation. \\n\\nKey components of the Transformer architecture include:\\n\\n1. **Encoder-Decoder Structure**: The Transformer consists of an encoder and a decoder, each made up of a stack of identical layers. The encoder processes the input data, while the decoder generates the output.\\n\\n2. **Stacked Layers**: The encoder is composed of N = 6 identical layers, each containing two main sub-layers:\\n   - A multi-head self-attention mechanism that allows the model to focus on different parts of the input sequence simultaneously.\\n   - A position-wise fully connected feed-forward network that processes the output of the attention mechanism.\\n\\n3. **Residual Connections and Layer Normalization**: Each sub-layer in the encoder and decoder includes a residual connection followed by layer normalization. This means that the output of each sub-layer is computed as LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. This design helps in stabilizing the training process and improving convergence.\\n\\n4. **Attention Mechanism**: The core innovation of the Transformer is its use of self-attention, which allows the model to weigh the importance of different words in a sequence when encoding or decoding. This mechanism enables the model to capture global dependencies without the limitations of sequential processing.\\n\\n5. **Parallelization**: The architecture allows for significant parallelization during training, as it does not rely on sequential data processing. This results in faster training times compared to traditional recurrent models.\\n\\n6. **Dimensionality**: The model operates with a fixed output dimension of d_model = 512 for all sub-layers and embedding layers, ensuring consistency across the architecture.\\n\\nOverall, the Transformer architecture has demonstrated superior performance in machine translation tasks, achieving state-of-the-art results while being more efficient in terms of training time and computational resources.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preguntar directamente\n",
    "rag.invoke(\"What is the Transformer architecture?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2ac331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Multi-head attention is a mechanism used in the Transformer model that allows the model to focus on different parts of the input sequence simultaneously by employing multiple attention heads. Each attention head operates independently and learns to attend to different representation subspaces of the input data. \n",
      "\n",
      "In multi-head attention, the input consists of queries (Q), keys (K), and values (V). Each attention head computes its own set of projections for these inputs using learned parameter matrices. Specifically, for each head \\(i\\), the queries, keys, and values are projected into lower-dimensional spaces defined by \\(d_k\\) and \\(d_v\\). The outputs of all heads are then concatenated and projected back to the original dimensionality \\(d_{model}\\) using another learned parameter matrix \\(W_O\\).\n",
      "\n",
      "The formula for multi-head attention can be expressed as:\n",
      "\n",
      "\\[\n",
      "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_O\n",
      "\\]\n",
      "\n",
      "where each head is computed as:\n",
      "\n",
      "\\[\n",
      "\\text{head}_i = \\text{Attention}(Q W_{Q_i}, K W_{K_i}, V W_{V_i})\n",
      "\\]\n",
      "\n",
      "This approach allows the model to capture various aspects of the input data by attending to different parts of the sequence in parallel, enhancing its ability to learn complex relationships and dependencies within the data. In the Transformer architecture, multi-head attention is utilized in both the encoder and decoder components, facilitating effective information flow and representation learning.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Multi-head attention is a mechanism used in the Transformer model that allows the model to focus on different parts of the input sequence simultaneously by employing multiple attention heads. Each attention head operates independently and learns to attend to different representation subspaces of the input data. \\n\\nIn multi-head attention, the input consists of queries (Q), keys (K), and values (V). Each attention head computes its own set of projections for these inputs using learned parameter matrices. Specifically, for each head \\\\(i\\\\), the queries, keys, and values are projected into lower-dimensional spaces defined by \\\\(d_k\\\\) and \\\\(d_v\\\\). The outputs of all heads are then concatenated and projected back to the original dimensionality \\\\(d_{model}\\\\) using another learned parameter matrix \\\\(W_O\\\\).\\n\\nThe formula for multi-head attention can be expressed as:\\n\\n\\\\[\\n\\\\text{MultiHead}(Q, K, V) = \\\\text{Concat}(\\\\text{head}_1, \\\\ldots, \\\\text{head}_h) W_O\\n\\\\]\\n\\nwhere each head is computed as:\\n\\n\\\\[\\n\\\\text{head}_i = \\\\text{Attention}(Q W_{Q_i}, K W_{K_i}, V W_{V_i})\\n\\\\]\\n\\nThis approach allows the model to capture various aspects of the input data by attending to different parts of the sequence in parallel, enhancing its ability to learn complex relationships and dependencies within the data. In the Transformer architecture, multi-head attention is utilized in both the encoder and decoder components, facilitating effective information flow and representation learning.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O con funci√≥n helper\n",
    "def ask(question, show_sources=False):\n",
    "    result = rag.invoke(question)\n",
    "    print(f\"üí° {result}\\n\")\n",
    "    if show_sources:\n",
    "        docs = retriever.invoke(question)\n",
    "        print(f\"üìö Fuentes ({len(docs)} docs):\")\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            print(f\"  {i}. P√°g {doc.metadata.get('page', '?')}\")\n",
    "    return result\n",
    "\n",
    "ask(\"What is multi-head attention?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
